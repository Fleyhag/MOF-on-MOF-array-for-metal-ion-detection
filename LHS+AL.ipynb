{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ff6628",
   "metadata": {},
   "source": [
    "Generate X_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701cb454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import csv\n",
    "\n",
    "values = list(range(0, 201, 10))\n",
    "\n",
    "with open('x_pool.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['x1', 'x2', 'x3', 'x4', 'x5'])\n",
    "    # 20^5 种笛卡尔积\n",
    "    for row in itertools.product(values, repeat=5):\n",
    "        writer.writerow(row)\n",
    "\n",
    "print('x_pool.csv generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7eb0c8",
   "metadata": {},
   "source": [
    "Gerate 192 initail data points using LHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5941890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skopt.sampler import Lhs\n",
    "from skopt.space import Space\n",
    "from scipy.spatial.distance import cdist\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sample_num = 192\n",
    "\n",
    "ion_levels = {\n",
    "    'Pb': np.linspace(0, 200, 21, dtype=int),\n",
    "    'Cd': np.linspace(0, 200, 21, dtype=int),\n",
    "    'Cu': np.linspace(0, 200, 21, dtype=int),\n",
    "    'Fe': np.linspace(0, 200, 21, dtype=int),\n",
    "    'K':  np.linspace(0, 200, 21, dtype=int)\n",
    "}\n",
    "\n",
    "df_pool = pd.read_csv('/mnt/nas_llm/zzb/cyg/al_test/x_pool.csv')\n",
    "\n",
    "X_pool = df_pool[['Pb', 'Cd', 'Cu', 'Fe', 'K']].values\n",
    "\n",
    "print(\"生成拉丁超立方样本\")\n",
    "space = Space([(0.0, 1.0)] * 5)\n",
    "sampler = Lhs(criterion='maximin')\n",
    "lhs_points = sampler.generate(space.dimensions, n_samples=sample_num)\n",
    "lhs_array = np.array(lhs_points) \n",
    "\n",
    "def continuous_to_discrete_point(cont_point, ion_levels):\n",
    "    \"\"\"\n",
    "    将 [0,1]^5 的连续点 映射为 最接近的离散浓度组合\n",
    "    cont_point: shape (5,) in [0,1]\n",
    "    返回: [Pb_val, Cd_val, Cu_val, Fe_val, K_val]\n",
    "    \"\"\"\n",
    "    ions = ['Pb', 'Cd', 'Cu', 'Fe', 'K']\n",
    "    discrete_point = []\n",
    "    for i, ion in enumerate(ions):\n",
    "        # 将 [0,1] 映射到实际浓度范围\n",
    "        min_val, max_val = ion_levels[ion][0], ion_levels[ion][-1]\n",
    "        real_val = cont_point[i] * (max_val - min_val) + min_val\n",
    "        # 找到离散水平中最接近的值\n",
    "        closest = ion_levels[ion][np.argmin(np.abs(ion_levels[ion] - real_val))]\n",
    "        discrete_point.append(closest)\n",
    "    return discrete_point\n",
    "\n",
    "print(\"映射到离散浓度\")\n",
    "discrete_candidates = []\n",
    "for point in lhs_array:\n",
    "    discrete_point = continuous_to_discrete_point(point, ion_levels)\n",
    "    discrete_candidates.append(discrete_point)\n",
    "\n",
    "df_candidates = pd.DataFrame(discrete_candidates, columns=['Pb', 'Cd', 'Cu', 'Fe', 'K'])\n",
    "print(f\"生成 {len(df_candidates)} 个目标离散点\")\n",
    "\n",
    "# 找到每个目标点欧氏距离最近邻\n",
    "print(\"计算距离\")\n",
    "dist_matrix = cdist(df_candidates.values, X_pool, metric='euclidean')\n",
    "closest_indices = np.argmin(dist_matrix, axis=1)  # 每行找最近的pool中的索引\n",
    "\n",
    "# 提取最终选中的行（保持顺序，去重）\n",
    "unique_indices = list(dict.fromkeys(closest_indices))  # 保持顺序去重\n",
    "df_initial = df_pool.iloc[unique_indices[:192]].copy()  # 取前n个\n",
    "\n",
    "print(\"\\n前5个选中的样本：\")\n",
    "print(df_initial.head())\n",
    "\n",
    "# 保存结果\n",
    "df_initial.to_csv('initial_192_samples.csv', index=False)\n",
    "print(\"\\n已保存到 'initial_192_samples.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c02956a",
   "metadata": {},
   "source": [
    "Iteratively generate next 16 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030e6134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load data\n",
    "# -----------------------------\n",
    "train_df = pd.read_csv(r\"previous_result.csv\", header=None) # wet experiment of previous 192/208/224 samples have been excuted\n",
    "X_train_np = train_df.iloc[:, 1:6].values.astype(np.float32)\n",
    "y_train_np = train_df.iloc[:, -3:].values.astype(np.float32)\n",
    "\n",
    "X_train = torch.tensor(X_train_np).to(device)\n",
    "y_train = torch.tensor(y_train_np).to(device)\n",
    "\n",
    "x_mean = X_train.mean(dim=0, keepdim=True)\n",
    "x_std = X_train.std(dim=0, keepdim=True) + 1e-8\n",
    "y_mean = y_train.mean(dim=0, keepdim=True)\n",
    "y_std = y_train.std(dim=0, keepdim=True) + 1e-8\n",
    "\n",
    "X_train_norm = (X_train - x_mean) / x_std\n",
    "y_train_norm = (y_train - y_mean) / y_std\n",
    "\n",
    "# -----------------------------\n",
    "# 2. def MLP model\n",
    "# -----------------------------\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=5, output_dim=3, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. model train (Deep Ensemble)\n",
    "# -----------------------------\n",
    "def train_model(X, y, epochs=2000, lr=1e-3):\n",
    "    model = MLP().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return model\n",
    "\n",
    "print(\"Training ensemble models...\")\n",
    "ensemble = []\n",
    "M = 7\n",
    "for i in range(M):\n",
    "    print(f\"Training model {i+1}/{M}\")\n",
    "    model = train_model(X_train_norm, y_train_norm, epochs=2500, lr=1e-3)\n",
    "    model.eval()\n",
    "    ensemble.append(model)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. load and predict x_pool\n",
    "# -----------------------------\n",
    "print(\"Loading x_pool...\")\n",
    "\n",
    "x_pool_df = pd.read_csv(\"x_pool.csv\", header=None)\n",
    "x_pool_np = x_pool_df.iloc[:, :5].values.astype(np.float32)\n",
    "x_pool = torch.tensor(x_pool_np).to(device)\n",
    "\n",
    "x_pool_norm = (x_pool - x_mean) / x_std\n",
    "\n",
    "# batched to avoid OOM\n",
    "batch_size = 20000\n",
    "n_samples = x_pool.shape[0]\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for model in ensemble:\n",
    "        preds = []\n",
    "        for i in tqdm(range(0, n_samples, batch_size), desc=f\"Predicting with model\"):\n",
    "            batch = x_pool_norm[i:i+batch_size]\n",
    "            out_norm = model(batch)\n",
    "            out = out_norm * y_std + y_mean  # 反标准化\n",
    "            preds.append(out.cpu().numpy())\n",
    "        all_preds.append(np.vstack(preds))\n",
    "\n",
    "\n",
    "all_preds = np.stack(all_preds, axis=-1)\n",
    "\n",
    "# calculate uncertainties\n",
    "uncertainties = np.std(all_preds, axis=-1).sum(axis=1)  # shape: (n_samples,)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. chose next 16 samples\n",
    "# -----------------------------\n",
    "top_16_idx = np.argsort(uncertainties)[-16:][::-1]\n",
    "selected_points = x_pool_np[top_16_idx]\n",
    "\n",
    "feature_names = ['Pb', 'Cd', 'Cu', 'Fe', 'K']\n",
    "selected_df = pd.DataFrame(selected_points, columns=feature_names)\n",
    "selected_df.to_csv(\"next_16_samples_AL_GPU.csv\", index=False)\n",
    "print(\"✅ Saved next_16_samples_AL_GPU.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
